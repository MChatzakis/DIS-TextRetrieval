{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2023-10-18T08:46:03.530394Z","iopub.execute_input":"2023-10-18T08:46:03.531235Z","iopub.status.idle":"2023-10-18T08:46:04.125834Z","shell.execute_reply.started":"2023-10-18T08:46:03.531197Z","shell.execute_reply":"2023-10-18T08:46:04.124718Z"},"trusted":true},"execution_count":1,"outputs":[{"name":"stdout","text":"/kaggle/input/queries-t2-expanded/test_queries_t2_expanded.jsonl\n/kaggle/input/dis-project-1-text-retrieval/sample_submission.csv\n/kaggle/input/dis-project-1-text-retrieval/task2_test.tsv\n/kaggle/input/dis-project-1-text-retrieval/task1_train.tsv\n/kaggle/input/dis-project-1-text-retrieval/task1_test.tsv\n/kaggle/input/dis-project-1-text-retrieval/task2_train.tsv\n/kaggle/input/queries-t1-expanded/test_queries_t1_expanded.jsonl\n/kaggle/input/tokenized-docs/tokenized_corpus.jsonl\n/kaggle/input/dis2023-project1-data/queries.jsonl\n/kaggle/input/dis2023-project1-data/corpus.jsonl\n","output_type":"stream"}]},{"cell_type":"code","source":"import pandas as pd\nimport json\nfrom nltk.stem import PorterStemmer, WordNetLemmatizer\nimport nltk\nfrom sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\nfrom sklearn.metrics.pairwise import linear_kernel\nimport string\nfrom nltk.corpus import stopwords\nimport math\nfrom operator import itemgetter\nfrom collections import Counter\nimport multiprocessing\n","metadata":{"execution":{"iopub.status.busy":"2023-10-18T08:46:04.127372Z","iopub.execute_input":"2023-10-18T08:46:04.127890Z","iopub.status.idle":"2023-10-18T08:46:06.150979Z","shell.execute_reply.started":"2023-10-18T08:46:04.127853Z","shell.execute_reply":"2023-10-18T08:46:06.149850Z"},"trusted":true},"execution_count":2,"outputs":[]},{"cell_type":"code","source":"def load_jsonl_data(data_path: str, key_name: str, value_name: str):\n    ids = []\n    texts = []\n    with open(data_path, 'r') as file:\n        for line in file:\n            data = json.loads(line)\n            ids.append(data[key_name])\n            texts.append(data[value_name])\n    return ids, texts","metadata":{"execution":{"iopub.status.busy":"2023-10-18T08:46:06.153285Z","iopub.execute_input":"2023-10-18T08:46:06.153662Z","iopub.status.idle":"2023-10-18T08:46:06.159737Z","shell.execute_reply.started":"2023-10-18T08:46:06.153626Z","shell.execute_reply":"2023-10-18T08:46:06.158694Z"},"trusted":true},"execution_count":3,"outputs":[]},{"cell_type":"code","source":"document_ids, documents = load_jsonl_data(\"/kaggle/input/dis2023-project1-data/corpus.jsonl\", \"_id\", \"text\")\nquery_ids, queries = load_jsonl_data(\"/kaggle/input/dis2023-project1-data/queries.jsonl\", \"_id\", \"text\")\ntokenized_document_ids, tokenized_documents = load_jsonl_data(\"/kaggle/input/tokenized-docs/tokenized_corpus.jsonl\", \"_id\", \"tokens\")","metadata":{"execution":{"iopub.status.busy":"2023-10-18T08:46:06.160976Z","iopub.execute_input":"2023-10-18T08:46:06.162161Z","iopub.status.idle":"2023-10-18T08:46:46.872767Z","shell.execute_reply.started":"2023-10-18T08:46:06.162123Z","shell.execute_reply":"2023-10-18T08:46:46.871655Z"},"trusted":true},"execution_count":4,"outputs":[]},{"cell_type":"code","source":"queries_t1_ids, queries_t1_tokens = load_jsonl_data(\"/kaggle/input/queries-t1-expanded/test_queries_t1_expanded.jsonl\", \"query_id\", \"tokens\")\nqueries_t2_ids, queries_t2_tokens = load_jsonl_data(\"/kaggle/input/queries-t2-expanded/test_queries_t2_expanded.jsonl\", \"query_id\", \"tokens\")","metadata":{"execution":{"iopub.status.busy":"2023-10-18T08:46:46.874631Z","iopub.execute_input":"2023-10-18T08:46:46.874945Z","iopub.status.idle":"2023-10-18T08:46:46.974006Z","shell.execute_reply.started":"2023-10-18T08:46:46.874919Z","shell.execute_reply":"2023-10-18T08:46:46.973139Z"},"trusted":true},"execution_count":5,"outputs":[]},{"cell_type":"code","source":"import math\nfrom six import iteritems\nfrom six.moves import range\nimport numpy as np\nimport heapq\nfrom collections.abc import Iterable\nfrom collections import defaultdict, Counter\n\n\n\nclass bm25(object):\n\n    def __init__(self, corpus_ids, corpus, k1=1.5, b=0.75, epsilon=0.25):\n        self.k1 = k1\n        self.b = b\n        self.epsilon = epsilon\n        self.corpus_size = 0\n        self.avg_doc_length = 0\n        self.doc_frequencies = []\n        self.idf = {}\n        self.doc_lengths = []\n        self.corpus = corpus\n        self.corpus_ids = corpus_ids\n\n    def fit(self):\n        term_to_freq = defaultdict(int)  \n        total_length = 0\n\n        for document in self.corpus:\n            self.corpus_size += 1\n            doc_length = len(document)\n            total_length += doc_length\n            self.doc_lengths.append(doc_length)\n\n            frequencies = Counter(document)\n            self.doc_frequencies.append(frequencies)\n\n            for term, freq in frequencies.items():\n                term_to_freq[term] += 1\n\n        self.avg_doc_length = float(total_length) / self.corpus_size\n        self.nd = term_to_freq\n\n        idf_sum = 0\n        idf_len = 0\n        negative_idfs = []\n\n        for word, freq in term_to_freq.items():\n            idf = math.log((self.corpus_size - freq + 0.5) / (freq + 0.5))\n            self.idf[word] = idf\n            idf_len += 1\n            idf_sum += idf\n            if idf < 0:\n                negative_idfs.append(word)\n\n        self.average_idf = idf_sum / idf_len\n        eps = self.epsilon * self.average_idf\n        self.idf.update({word: eps for word in negative_idfs})\n\n        document_score = {}\n        for i, document in enumerate(self.corpus):\n            doc_freqs = self.doc_frequencies[i]\n            for word in document:\n                if word not in doc_freqs:\n                    continue\n                score = (self.idf[word] * doc_freqs[word] * (self.k1 + 1)\n                          / (doc_freqs[word] + self.k1 * (1 - self.b + self.b * self.doc_lengths[i] / self.avg_doc_length)))\n                if word not in document_score:\n                    document_score[word] = {i: round(score, 2)}\n                else:\n                    document_score[word].update({i: round(score, 2)})\n        self.document_score = document_score\n\n\n    def compute_similarity(self, query, doc):\n        score = 0\n        doc_freqs = Counter(query)\n        freq = 1\n        default_idf = math.log(self.corpus_size - freq + 0.5) - math.log(freq + 0.5)\n        for word in doc:\n            if word not in doc_freqs:\n                continue\n            score += (self.idf.get(word,default_idf) * doc_freqs[word] * (self.k1 + 1)\n                      / (doc_freqs[word] + self.k1 * (1 - self.b + self.b * len(query) / self.avg_doc_length)))\n        return score\n\n        \n    def get_top_k_documents(self,document,k=1):\n        score_overall = {}\n        for word in document:\n            if word not in self.document_score:\n                continue\n            for key, value in self.document_score[word].items():\n                score_overall[key] = score_overall.get(key, 0) + value\n\n        k_keys_sorted = heapq.nlargest(k, score_overall,key=score_overall.get)\n        return [(score_overall.get(item,None), self.corpus_ids[item], self.corpus[item]) for item in k_keys_sorted]","metadata":{"execution":{"iopub.status.busy":"2023-10-18T08:47:30.780744Z","iopub.execute_input":"2023-10-18T08:47:30.781166Z","iopub.status.idle":"2023-10-18T08:47:30.801880Z","shell.execute_reply.started":"2023-10-18T08:47:30.781138Z","shell.execute_reply":"2023-10-18T08:47:30.801061Z"},"trusted":true},"execution_count":6,"outputs":[]},{"cell_type":"code","source":"import pandas as pd\nimport numpy as np\n\nimport json\nimport random\nimport string\nimport Levenshtein\n\nimport nltk\nfrom nltk.corpus import stopwords\nfrom nltk.corpus import wordnet\nfrom nltk.tokenize import word_tokenize\nfrom nltk.stem import PorterStemmer\n\nfrom tqdm import tqdm\n\nnltk.download(\"wordnet\")\n\nrandom.seed(0)\n\n\nclass Expander:\n    def __init__(self, add_synonym_prob=0.5, levenshtein_th = 2):\n        self.add_synonym_prob = add_synonym_prob\n        self.levenshtein_th = levenshtein_th\n\n    def expand(self, word_list):\n        expanded_word_list = []\n        for word in word_list:\n            synonym = self.get_synonym(word)\n            synonym_lower = synonym.lower()\n            if (\n                random.random() < self.add_synonym_prob\n                and Levenshtein.distance(synonym_lower, word) > self.levenshtein_th\n            ):\n                expanded_word_list.append(synonym_lower)\n                #print(\"Synonym added: {} -> {}\".format(word, synonym))\n\n        return word_list + expanded_word_list\n\n    def get_synonym(self, word):\n        synonyms = set()\n\n        for syn in wordnet.synsets(word):\n            for lemma in syn.lemmas():\n                if lemma.name().isalpha():\n                    synonyms.add(lemma.name())\n                    break\n\n        synonyms = list(synonyms)\n        return synonyms[0] if len(synonyms) > 0 else word\n","metadata":{"execution":{"iopub.status.busy":"2023-10-18T08:47:31.355569Z","iopub.execute_input":"2023-10-18T08:47:31.356184Z","iopub.status.idle":"2023-10-18T08:47:31.588316Z","shell.execute_reply.started":"2023-10-18T08:47:31.356148Z","shell.execute_reply":"2023-10-18T08:47:31.587262Z"},"trusted":true},"execution_count":7,"outputs":[{"name":"stdout","text":"[nltk_data] Downloading package wordnet to /usr/share/nltk_data...\n[nltk_data]   Package wordnet is already up-to-date!\n","output_type":"stream"}]},{"cell_type":"code","source":"import pandas as pd\nimport numpy as np\nimport json\nimport string\n\nimport nltk\nfrom nltk.corpus import stopwords\nfrom nltk.tokenize import word_tokenize\nfrom nltk.stem import PorterStemmer\n\nfrom tqdm import tqdm\n\nnltk.download(\"stopwords\")\nnltk.download(\"punkt\")\n\nclass Preprocessor:\n    def __init__(self, expander=None):\n        self.stemmer = PorterStemmer()\n        self.stopwords = set(stopwords.words(\"english\"))\n        self.punctuation = set(string.punctuation)\n        self.expander = expander\n\n    def preprocess(self, documents):\n        tokenized_docs = []\n        if isinstance(documents, list):\n            tokenized_docs = self.preprocess_document_list(documents)\n        elif isinstance(documents, dict):\n            tokenized_docs = self.preprocess_document_dict(documents)\n        else:\n            raise TypeError(\"Documents must be either a list or a dictionary\")\n\n        return tokenized_docs\n    \n    def preprocess_query(self, query, expand=False):\n        query = self.tolowercase(query)\n        query = self.remove_punctuation(query)\n        \n        query_tokens = self.tokenize(query)\n        query_tokens = self.remove_stopwords(query_tokens)\n        \n        if expand:\n            query_tokens = self.expand(query_tokens)\n        \n        query_tokens = self.stem(query_tokens)\n        \n        return query_tokens\n    \n    def preprocess_document_list(self, document_list):\n        tokenized_docs = []\n        for i in tqdm(range(len(document_list))):\n            tokenized_docs.append(self.preprocess_doc(document_list[i]))\n        return tokenized_docs\n\n    def preprocess_document_dict(self, document_dict):\n        tokenized_docs = {}\n        for doc_id in tqdm(document_dict.keys()):\n            document = document_dict[doc_id]\n            tokenized_docs[doc_id] = self.preprocess_doc(document)\n        return tokenized_docs\n            \n    def preprocess_doc(self, document):\n        document = self.tolowercase(document)\n        document = self.remove_punctuation(document)\n        \n        document_tokens = self.tokenize(document)\n        document_tokens = self.remove_stopwords(document_tokens)\n        document_tokens = self.stem(document_tokens)\n        \n        return document_tokens\n\n    def tolowercase(self, document):\n        return document.lower()\n\n    def remove_punctuation(self, document):\n        return \"\".join([char for char in document if char not in self.punctuation])\n\n    def tokenize(self, document):\n        return word_tokenize(document)\n\n    def remove_stopwords(self, tokens):\n        return [token for token in tokens if token not in self.stopwords]\n\n    def stem(self, tokens):\n        return [self.stemmer.stem(token) for token in tokens]\n\n    def save_docs(self, docs, path):\n        with open(path, 'w') as jsonl_file:\n            for docID in docs:\n                doc_data = {\"_id\": str(docID), \"tokens\": docs[docID]}\n                json_line = json.dumps(doc_data)\n                jsonl_file.write(json_line + '\\n')\n            \n    def load_docs(self, path):\n        raw_queries = {}\n        with open(path, \"r\") as file:\n            for line in file:\n                data = json.loads(line)\n                raw_queries[data[\"_id\"]] = data[\"tokens\"]\n    \n        return raw_queries\n    \n    def expand(self, terms):\n        return self.expander.expand(terms)","metadata":{"execution":{"iopub.status.busy":"2023-10-18T08:47:31.939294Z","iopub.execute_input":"2023-10-18T08:47:31.940111Z","iopub.status.idle":"2023-10-18T08:47:32.053519Z","shell.execute_reply.started":"2023-10-18T08:47:31.940068Z","shell.execute_reply":"2023-10-18T08:47:32.052735Z"},"trusted":true},"execution_count":8,"outputs":[{"name":"stdout","text":"[nltk_data] Downloading package stopwords to /usr/share/nltk_data...\n[nltk_data]   Package stopwords is already up-to-date!\n[nltk_data] Downloading package punkt to /usr/share/nltk_data...\n[nltk_data]   Package punkt is already up-to-date!\n","output_type":"stream"}]},{"cell_type":"code","source":"nltk.download('wordnet')\n","metadata":{"execution":{"iopub.status.busy":"2023-10-17T14:32:52.504532Z","iopub.execute_input":"2023-10-17T14:32:52.505217Z","iopub.status.idle":"2023-10-17T14:32:52.515129Z","shell.execute_reply.started":"2023-10-17T14:32:52.505177Z","shell.execute_reply":"2023-10-17T14:32:52.513796Z"},"trusted":true},"execution_count":16,"outputs":[{"name":"stdout","text":"[nltk_data] Downloading package wordnet to /usr/share/nltk_data...\n[nltk_data]   Package wordnet is already up-to-date!\n","output_type":"stream"},{"execution_count":16,"output_type":"execute_result","data":{"text/plain":"True"},"metadata":{}}]},{"cell_type":"code","source":"import json\nimport sys\nimport random\nimport gc\n\nrandom.seed(0)\nnp.random.seed(0)\n\npreprocessor = Preprocessor(expander=Expander())\ntokenized_query = preprocessor.preprocess_query(\"topics\", expand=True)\n\ntokenized_query","metadata":{"execution":{"iopub.status.busy":"2023-10-17T14:29:21.013820Z","iopub.status.idle":"2023-10-17T14:29:21.015077Z","shell.execute_reply.started":"2023-10-17T14:29:21.014857Z","shell.execute_reply":"2023-10-17T14:29:21.014883Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model = bm25(tokenized_document_ids, tokenized_documents)","metadata":{"execution":{"iopub.status.busy":"2023-10-18T08:47:36.766285Z","iopub.execute_input":"2023-10-18T08:47:36.766672Z","iopub.status.idle":"2023-10-18T08:47:36.771747Z","shell.execute_reply.started":"2023-10-18T08:47:36.766643Z","shell.execute_reply":"2023-10-18T08:47:36.770523Z"},"trusted":true},"execution_count":9,"outputs":[]},{"cell_type":"code","source":"model.fit()","metadata":{"execution":{"iopub.status.busy":"2023-10-18T08:47:37.584943Z","iopub.execute_input":"2023-10-18T08:47:37.585342Z","iopub.status.idle":"2023-10-18T08:50:15.087341Z","shell.execute_reply.started":"2023-10-18T08:47:37.585314Z","shell.execute_reply":"2023-10-18T08:50:15.086174Z"},"trusted":true},"execution_count":10,"outputs":[]},{"cell_type":"code","source":"import time\nquery = \"____________________ is considered the father of modern medicine.\"\n#tokenized_query = preprocessor.preprocess([query])[0]\n#print(tokenized_query)\ntokenized_query=[\"consid\", \"father\", \"modern\", \"medicin\", \"advanc\"]\n# Record the starting time\nstart_time = time.time()\nresult = model.get_top_k_documents(tokenized_query,k=10)\n# Record the ending time\nend_time = time.time()\n\n# Calculate the elapsed time\nelapsed_time = end_time - start_time\n\n# Print the elapsed time in seconds\nprint(f\"Time taken: {elapsed_time:.6f} seconds\")\nprint(result)\n","metadata":{"execution":{"iopub.status.busy":"2023-10-17T15:27:32.220393Z","iopub.execute_input":"2023-10-17T15:27:32.220833Z","iopub.status.idle":"2023-10-17T15:27:32.299551Z","shell.execute_reply.started":"2023-10-17T15:27:32.220800Z","shell.execute_reply":"2023-10-17T15:27:32.298769Z"},"trusted":true},"execution_count":39,"outputs":[{"name":"stdout","text":"Time taken: 0.073173 seconds\n[(25.619999999999997, '7067274', ['true', 'hippocr', 'consid', 'father', 'modern', 'medicin', 'believ', 'ill', 'punish', 'inflict', 'god', 'true', 'fals', 'weegi', 'true']), (24.24, '6221776', ['key', 'advanc', 'invent', 'first', 'liquidfuel', 'rocket', '1926', 'american', 'robert', 'goddard', 'auburn', 'massachusett', 'goddard', 'consid', 'one', 'father', 'modern', 'rocket', 'era']), (23.619999999999997, '1195004', ['rate', 'newest', 'oldest', 'best', 'answer', 'rudolf', 'ludwig', 'karl', 'virchow', '13', 'octob', '1821', 'â\\x80\\x93', '5', 'septemb', '1902', 'german', 'doctor', 'anthropologist', 'pathologist', 'prehistorian', 'biologist', 'politician', 'known', 'advanc', 'public', 'health', 'refer', 'father', 'modern', 'patholog', 'consid', 'one', 'founder', 'social', 'medicin']), (23.45, '1176428', ['niccolã²', 'machiavelli', 'consid', 'father', 'modern', 'polit', 'scienc', 'book', 'princ', 'one', 'first', 'work', 'modern', 'polit', 'philosophi', 'modern', 'philosophi', 'niccolã²', 'machiavelli', 'consid', 'father', 'modern', 'polit', 'scienc', 'book', 'princ', 'one', 'first', 'work', 'modern', 'polit', 'philosophi', 'modern', 'philosophi']), (21.150000000000002, '5066808', ['edgar', 'cayc', 'consid', 'mani', 'father', 'modern', 'holist', 'medicin', 'edgar', 'cayc', 'produc', 'tremend', 'legaci', 'inform', 'human', 'bodi', 'inform', 'includ', 'spiritu', 'principl', 'medit', 'health', 'tonic', 'benefici', 'oil', 'rejuven', 'remedi']), (20.990000000000002, '3334985', ['transcript', 'john', 'dalton', 'father', 'modern', 'atom', 'theori', 'john', 'dalton', 'father', 'modern', 'atom', 'theorybi', 'luciana', 'villarr', '8a', '23due', 'date', 'may', '122014', 'modern', 'age', 'year', 'ago', '1750', '1850', 'thousand', 'chang', 'world', 'improv', 'discoveri', 'technolog', 'advanc', 'period', '100', 'year', 'known', 'modern', 'age']), (19.38, '5110891', ['question', 'necessarili', 'cutanddri', 'answer', 'sinc', 'mani', 'individu', 'contribut', 'incept', 'rise', 'evolut', 'modern', 'day', 'psycholog', 'well', 'take', 'closer', 'look', 'singl', 'individu', 'often', 'cite', 'well', 'individu', 'also', 'consid', 'father', 'psycholog', 'father', 'modern', 'psycholog', 'wilhelm', 'wundt', 'man', 'commonli', 'identifi', 'father', 'psycholog']), (19.189999999999998, '3935658', ['achiev', 'includ', 'improv', 'telescop', 'consequ', 'astronom', 'observ', 'support', 'heliocentr', 'galileo', 'call', 'father', 'modern', 'observ', 'astronomi', 'father', 'modern', 'physic', 'father', 'modern', 'scienc']), (18.88, '3772576', ['persist', 'effort', 'determin', 'cholera', 'spread', 'statist', 'map', 'method', 'initi', 'john', 'snow', 'wide', 'consid', 'father', 'modern', 'epidemiolog']), (18.44, '3485264', ['sigmund', 'freud', 'vienna', 'consid', 'father', 'modern', 'psycholog', 'origin', 'new', 'method', 'call', 'free', 'associ', 'techniqu', 'freud', 'consid', 'mind', 'three', 'part', 'consciou', 'preconsci', 'unconsci', 'consid', '90', 'mind', 'unconsci', 'mind'])]\n","output_type":"stream"}]},{"cell_type":"code","source":"data = []\nfor id, tokens in enumerate(queries_t1_tokens):\n    \n    result = model.get_top_k_documents(tokens,k=10)\n    \n    corpus_ids = []\n    for r in result:\n        corpus_ids.append(r[1])\n    \n    data.append((id, corpus_ids, -1))","metadata":{"execution":{"iopub.status.busy":"2023-10-18T08:53:44.261176Z","iopub.execute_input":"2023-10-18T08:53:44.261585Z","iopub.status.idle":"2023-10-18T09:37:40.958530Z","shell.execute_reply.started":"2023-10-18T08:53:44.261555Z","shell.execute_reply":"2023-10-18T09:37:40.957213Z"},"trusted":true},"execution_count":11,"outputs":[]},{"cell_type":"code","source":"import csv\ncsv_file = \"output1.csv\"\n\nwith open(csv_file, mode=\"w\", newline=\"\") as file:\n    writer = csv.writer(file)\n    writer.writerow([\"id\", \"corpus-id\", \"score\"])\n    writer.writerows(data)\n\nprint(f\"CSV file '{csv_file}' has been created.\")","metadata":{"execution":{"iopub.status.busy":"2023-10-18T09:37:48.939760Z","iopub.execute_input":"2023-10-18T09:37:48.940171Z","iopub.status.idle":"2023-10-18T09:37:49.008054Z","shell.execute_reply.started":"2023-10-18T09:37:48.940143Z","shell.execute_reply":"2023-10-18T09:37:49.006264Z"},"trusted":true},"execution_count":12,"outputs":[{"name":"stdout","text":"CSV file 'output1.csv' has been created.\n","output_type":"stream"}]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}