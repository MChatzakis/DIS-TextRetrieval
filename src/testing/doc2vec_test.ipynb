{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Basic functions and usage of Doc2Vec wrapper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import sys\n",
    "\n",
    "sys.path.append(\"../\")\n",
    "\n",
    "from gensim.models.doc2vec import Doc2Vec, TaggedDocument\n",
    "from models.doc2vec_model import Doc2VecModel\n",
    "from tqdm import tqdm\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import IR_utils\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load the corpus data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of documents in corpus: 1471406\n"
     ]
    }
   ],
   "source": [
    "data_path = \"../../data/dataset/corpus.jsonl\"\n",
    "max_docs = -1\n",
    "docs = {}\n",
    "with open(data_path, \"r\") as file:\n",
    "    for line in file:\n",
    "        data = json.loads(line)\n",
    "        docs[data[\"_id\"]] = data[\"text\"]\n",
    "\n",
    "        if max_docs > 0 and len(docs) == max_docs:\n",
    "            break\n",
    "\n",
    "print(\"Number of documents in corpus: {}\".format(len(docs)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocess the documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Todo!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train or Load the Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training (disabled)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "if False:\n",
    "    vector_size = 10\n",
    "    window = 5\n",
    "    min_count = 60\n",
    "    workers = 16\n",
    "    epochs = 20\n",
    "    d2v = Doc2VecModel(\n",
    "        docs,\n",
    "        vector_size=vector_size,\n",
    "        window=window,\n",
    "        min_count=min_count,\n",
    "        workers=workers,\n",
    "        epochs=epochs,\n",
    "    )\n",
    "\n",
    "    d2v.fit()\n",
    "\n",
    "    d2v.save(\n",
    "        f\"../../models/doc2vec.docs{len(d2v.model.dv)}.vs{vector_size}.win{window}.min{min_count}.ep{epochs}.model\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load the model from disk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "vector_size = 20\n",
    "window = 10\n",
    "min_count = 60\n",
    "workers = 16\n",
    "epochs = 20\n",
    "num_docs = len(docs)\n",
    "\n",
    "path = f\"../../models/doc2vec.docs{num_docs}.vs{vector_size}.win{window}.min{min_count}.ep{epochs}.model\"\n",
    "\n",
    "d2v = Doc2VecModel(path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load the queries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of queries: 509962\n"
     ]
    }
   ],
   "source": [
    "# Load the query data\n",
    "query_data_path = \"../../data/dataset/queries.jsonl\"\n",
    "raw_queries = {}\n",
    "with open(query_data_path, \"r\") as file:\n",
    "    for line in file:\n",
    "        data = json.loads(line)\n",
    "        raw_queries[int(data[\"_id\"])] = data[\"text\"]\n",
    "\n",
    "print(\"Number of queries: {}\".format(len(raw_queries)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of queries: 502939\n"
     ]
    }
   ],
   "source": [
    "query_ids_df = pd.read_csv(\"../../data/task1_train.tsv\", delimiter=\"\\t\")\n",
    "grouped_queries = query_ids_df.groupby(\"query-id\")\n",
    "\n",
    "queries = {}\n",
    "for query_id, group in grouped_queries:\n",
    "    relevant_doc_ids = group[\"corpus-id\"].tolist()\n",
    "    scores = group[\"score\"].tolist()\n",
    "\n",
    "    query_text = raw_queries[query_id]\n",
    "\n",
    "    queries[query_id] = {\n",
    "        \"text\": query_text,\n",
    "        \"relevant_doc_ids\": relevant_doc_ids,\n",
    "        \"relevant_doc_scores\": scores,\n",
    "    }\n",
    "\n",
    "print(\"Number of queries: {}\".format(len(queries)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 1804/502939 [00:41<3:28:17, 40.10it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.1 1.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  1%|▏         | 7317/502939 [02:32<2:42:04, 50.97it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.1 1.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  2%|▏         | 9637/502939 [03:17<2:48:53, 48.68it/s]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m/Users/manoschatzakis/Documents/GitHub/DIS-TextRetrieval/src/testing/doc2vec_test.ipynb Cell 16\u001b[0m line \u001b[0;36m4\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/manoschatzakis/Documents/GitHub/DIS-TextRetrieval/src/testing/doc2vec_test.ipynb#W4sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m TOP_K \u001b[39m=\u001b[39m \u001b[39m10\u001b[39m\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/manoschatzakis/Documents/GitHub/DIS-TextRetrieval/src/testing/doc2vec_test.ipynb#W4sZmlsZQ%3D%3D?line=2'>3</a>\u001b[0m \u001b[39mfor\u001b[39;00m query_id \u001b[39min\u001b[39;00m tqdm(queries\u001b[39m.\u001b[39mkeys()):\n\u001b[0;32m----> <a href='vscode-notebook-cell:/Users/manoschatzakis/Documents/GitHub/DIS-TextRetrieval/src/testing/doc2vec_test.ipynb#W4sZmlsZQ%3D%3D?line=3'>4</a>\u001b[0m     d2v_query_answers \u001b[39m=\u001b[39m d2v\u001b[39m.\u001b[39mfind_similar(queries[query_id][\u001b[39m\"\u001b[39m\u001b[39mtext\u001b[39m\u001b[39m\"\u001b[39m], TOP_K)\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/manoschatzakis/Documents/GitHub/DIS-TextRetrieval/src/testing/doc2vec_test.ipynb#W4sZmlsZQ%3D%3D?line=5'>6</a>\u001b[0m     retrieved_doc_ids \u001b[39m=\u001b[39m []\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/manoschatzakis/Documents/GitHub/DIS-TextRetrieval/src/testing/doc2vec_test.ipynb#W4sZmlsZQ%3D%3D?line=6'>7</a>\u001b[0m     retrieved_doc_scores \u001b[39m=\u001b[39m []\n",
      "File \u001b[0;32m~/Documents/GitHub/DIS-TextRetrieval/src/testing/../models/doc2vec_model.py:57\u001b[0m, in \u001b[0;36mDoc2VecModel.find_similar\u001b[0;34m(self, query, topk)\u001b[0m\n\u001b[1;32m     55\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mfind_similar\u001b[39m(\u001b[39mself\u001b[39m, query, topk\u001b[39m=\u001b[39m\u001b[39m5\u001b[39m):\n\u001b[1;32m     56\u001b[0m     vector \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmodel\u001b[39m.\u001b[39minfer_vector(query\u001b[39m.\u001b[39msplit())\n\u001b[0;32m---> 57\u001b[0m     similar_documents \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmodel\u001b[39m.\u001b[39mdocvecs\u001b[39m.\u001b[39mmost_similar(\n\u001b[1;32m     58\u001b[0m         positive\u001b[39m=\u001b[39m[vector], topn\u001b[39m=\u001b[39mtopk\n\u001b[1;32m     59\u001b[0m     )\n\u001b[1;32m     61\u001b[0m     similar_documents \u001b[39m=\u001b[39m [(\u001b[39mint\u001b[39m(docID), similarity) \u001b[39mfor\u001b[39;00m docID, similarity \u001b[39min\u001b[39;00m similar_documents]\n\u001b[1;32m     63\u001b[0m     \u001b[39mreturn\u001b[39;00m similar_documents\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/gensim/models/keyedvectors.py:852\u001b[0m, in \u001b[0;36mKeyedVectors.most_similar\u001b[0;34m(self, positive, negative, topn, clip_start, clip_end, restrict_vocab, indexer)\u001b[0m\n\u001b[1;32m    850\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m topn:\n\u001b[1;32m    851\u001b[0m     \u001b[39mreturn\u001b[39;00m dists\n\u001b[0;32m--> 852\u001b[0m best \u001b[39m=\u001b[39m matutils\u001b[39m.\u001b[39margsort(dists, topn\u001b[39m=\u001b[39mtopn \u001b[39m+\u001b[39m \u001b[39mlen\u001b[39m(all_keys), reverse\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m)\n\u001b[1;32m    853\u001b[0m \u001b[39m# ignore (don't return) keys from the input\u001b[39;00m\n\u001b[1;32m    854\u001b[0m result \u001b[39m=\u001b[39m [\n\u001b[1;32m    855\u001b[0m     (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mindex_to_key[sim \u001b[39m+\u001b[39m clip_start], \u001b[39mfloat\u001b[39m(dists[sim]))\n\u001b[1;32m    856\u001b[0m     \u001b[39mfor\u001b[39;00m sim \u001b[39min\u001b[39;00m best \u001b[39mif\u001b[39;00m (sim \u001b[39m+\u001b[39m clip_start) \u001b[39mnot\u001b[39;00m \u001b[39min\u001b[39;00m all_keys\n\u001b[1;32m    857\u001b[0m ]\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/gensim/matutils.py:77\u001b[0m, in \u001b[0;36margsort\u001b[0;34m(x, topn, reverse)\u001b[0m\n\u001b[1;32m     75\u001b[0m     \u001b[39mreturn\u001b[39;00m np\u001b[39m.\u001b[39margsort(x)[:topn]\n\u001b[1;32m     76\u001b[0m \u001b[39m# np >= 1.8 has a fast partial argsort, use that!\u001b[39;00m\n\u001b[0;32m---> 77\u001b[0m most_extreme \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39margpartition(x, topn)[:topn]\n\u001b[1;32m     78\u001b[0m \u001b[39mreturn\u001b[39;00m most_extreme\u001b[39m.\u001b[39mtake(np\u001b[39m.\u001b[39margsort(x\u001b[39m.\u001b[39mtake(most_extreme)))\n",
      "File \u001b[0;32m<__array_function__ internals>:200\u001b[0m, in \u001b[0;36margpartition\u001b[0;34m(*args, **kwargs)\u001b[0m\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/numpy/core/fromnumeric.py:871\u001b[0m, in \u001b[0;36margpartition\u001b[0;34m(a, kth, axis, kind, order)\u001b[0m\n\u001b[1;32m    792\u001b[0m \u001b[39m@array_function_dispatch\u001b[39m(_argpartition_dispatcher)\n\u001b[1;32m    793\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39margpartition\u001b[39m(a, kth, axis\u001b[39m=\u001b[39m\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m, kind\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39mintroselect\u001b[39m\u001b[39m'\u001b[39m, order\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m):\n\u001b[1;32m    794\u001b[0m \u001b[39m    \u001b[39m\u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m    795\u001b[0m \u001b[39m    Perform an indirect partition along the given axis using the\u001b[39;00m\n\u001b[1;32m    796\u001b[0m \u001b[39m    algorithm specified by the `kind` keyword. It returns an array of\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    869\u001b[0m \n\u001b[1;32m    870\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 871\u001b[0m     \u001b[39mreturn\u001b[39;00m _wrapfunc(a, \u001b[39m'\u001b[39m\u001b[39margpartition\u001b[39m\u001b[39m'\u001b[39m, kth, axis\u001b[39m=\u001b[39maxis, kind\u001b[39m=\u001b[39mkind, order\u001b[39m=\u001b[39morder)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/numpy/core/fromnumeric.py:57\u001b[0m, in \u001b[0;36m_wrapfunc\u001b[0;34m(obj, method, *args, **kwds)\u001b[0m\n\u001b[1;32m     54\u001b[0m     \u001b[39mreturn\u001b[39;00m _wrapit(obj, method, \u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwds)\n\u001b[1;32m     56\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m---> 57\u001b[0m     \u001b[39mreturn\u001b[39;00m bound(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwds)\n\u001b[1;32m     58\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mTypeError\u001b[39;00m:\n\u001b[1;32m     59\u001b[0m     \u001b[39m# A TypeError occurs if the object does have such a method in its\u001b[39;00m\n\u001b[1;32m     60\u001b[0m     \u001b[39m# class, but its signature is not identical to that of NumPy's. This\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     64\u001b[0m     \u001b[39m# Call _wrapit from within the except clause to ensure a potential\u001b[39;00m\n\u001b[1;32m     65\u001b[0m     \u001b[39m# exception has a traceback chain.\u001b[39;00m\n\u001b[1;32m     66\u001b[0m     \u001b[39mreturn\u001b[39;00m _wrapit(obj, method, \u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwds)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "TOP_K = 10\n",
    "\n",
    "for query_id in tqdm(queries.keys()):\n",
    "    d2v_query_answers = d2v.find_similar(queries[query_id][\"text\"], TOP_K)\n",
    "\n",
    "    retrieved_doc_ids = []\n",
    "    retrieved_doc_scores = []\n",
    "    for doc_id, score in d2v_query_answers:\n",
    "        retrieved_doc_ids.append(doc_id)\n",
    "        retrieved_doc_scores.append(score)\n",
    "\n",
    "    queries[query_id][\"retrieved_doc_ids\"] = retrieved_doc_ids\n",
    "    queries[query_id][\"retrieved_doc_scores\"] = retrieved_doc_scores\n",
    "\n",
    "    queries[query_id][\"precision@10\"] = IR_utils.precision_K(\n",
    "        retrieved_docs=retrieved_doc_ids,\n",
    "        relevant_docs=queries[query_id][\"relevant_doc_ids\"],\n",
    "        K=10,\n",
    "    )\n",
    "    \n",
    "    queries[query_id][\"recall@10\"] = IR_utils.recall_K(\n",
    "        retrieved_docs=retrieved_doc_ids,\n",
    "        relevant_docs=queries[query_id][\"relevant_doc_ids\"],\n",
    "        K=10,\n",
    "    )\n",
    "    \n",
    "    #print(retrieved_doc_ids)\n",
    "    #print(queries[query_id][\"relevant_doc_ids\"])\n",
    "    #print(queries[query_id][\"precision@10\"], queries[query_id][\"recall@10\"])\n",
    "\n",
    "    if queries[query_id][\"precision@10\"] > 0.0:\n",
    "        print(queries[query_id][\"precision@10\"], queries[query_id][\"recall@10\"])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
